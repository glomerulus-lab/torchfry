

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torchfry.transforms &mdash; Torched and Fried Spring 2025 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=c4a7d8b7"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="torchfry.networks" href="torchfry.networks.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Torched and Fried
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torchfry.networks.html">torchfry.networks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torchfry.transforms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-torchfry.transforms">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#torchfry.transforms.FastfoodLayer"><code class="docutils literal notranslate"><span class="pre">FastfoodLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#torchfry.transforms.FastfoodLayer.forward"><code class="docutils literal notranslate"><span class="pre">FastfoodLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#torchfry.transforms.FastfoodLayer.new_feature_map"><code class="docutils literal notranslate"><span class="pre">FastfoodLayer.new_feature_map()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#torchfry.transforms.FastfoodLayer.phi"><code class="docutils literal notranslate"><span class="pre">FastfoodLayer.phi()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#torchfry.transforms.RKSLayer"><code class="docutils literal notranslate"><span class="pre">RKSLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#torchfry.transforms.RKSLayer.forward"><code class="docutils literal notranslate"><span class="pre">RKSLayer.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#torchfry.transforms.RKSLayer.phi"><code class="docutils literal notranslate"><span class="pre">RKSLayer.phi()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Torched and Fried</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">torchfry.transforms</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/torchfry.transforms.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="torchfry-transforms">
<h1>torchfry.transforms<a class="headerlink" href="#torchfry-transforms" title="Link to this heading"></a></h1>
<section id="module-torchfry.transforms">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-torchfry.transforms" title="Link to this heading"></a></h2>
<p>Implementation of Fastfood and Random Kitchen Sink custom PyTorch layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="torchfry.transforms.FastfoodLayer">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchfry.transforms.</span></span><span class="sig-name descname"><span class="pre">FastfoodLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_S</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_G</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_B</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nonlinearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hadamard</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/FastfoodLayer.html#FastfoodLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.FastfoodLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implementation of Fastfood transformation layer for efficient random feature mapping.</p>
<p>This layer approximates a dense random projection using the Fastfood algorithm,
which utilizes structured matrices (Hadamard, diagonal random, permutation matrices)
to reduce time complexity from Random Kitchen Sink’s <span class="math notranslate nohighlight">\(O(nd)\)</span> to
<span class="math notranslate nohighlight">\(O(n \log d)\)</span> and space complexity from <span class="math notranslate nohighlight">\(O(n^2)\)</span> to <span class="math notranslate nohighlight">\(O(n)\)</span>, where
<span class="math notranslate nohighlight">\(d\)</span> is the input_dim and <span class="math notranslate nohighlight">\(n\)</span> is the output_dim.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – The input data feature dimension. (<span class="math notranslate nohighlight">\(d\)</span>)</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – The output dimension to be projected into. (<span class="math notranslate nohighlight">\(n\)</span>)</p></li>
<li><p><strong>scale</strong> (<em>float</em>) – Scalar factor for normalization. (<span class="math notranslate nohighlight">\(\sigma\)</span>)</p></li>
<li><p><strong>learn_S</strong> (<em>bool</em>) – If <span class="math notranslate nohighlight">\(S\)</span> matrix is to be learnable.</p></li>
<li><p><strong>learn_G</strong> (<em>bool</em>) – If <span class="math notranslate nohighlight">\(G\)</span> matrix is to be learnable.</p></li>
<li><p><strong>learn_B</strong> (<em>bool</em>) – If <span class="math notranslate nohighlight">\(B\)</span> matrix is to be learnable.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device on which computations will be performed.</p></li>
<li><p><strong>nonlinearity</strong> (<em>bool</em>) – If True, apply nonlinearity of <span class="math notranslate nohighlight">\(cos(Vx + u)\)</span>.</p></li>
<li><p><strong>hadamard</strong> (<em>str</em>) – Type of hadamard function desired: Dao, matrix multiplication, or Recursive
FWHT (<code class="docutils literal notranslate"><span class="pre">Dao</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code>).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<div class="math notranslate nohighlight">
\[Vx = \frac{1}{\sigma \sqrt{d}} SHG \Pi HB\]</div>
<p><span class="math notranslate nohighlight">\(S\)</span>: Diagonal scaling matrix, allows our rows of <span class="math notranslate nohighlight">\(V\)</span> to be independent of
one another. For Fastfood, this helps us match the radial shape from an RBF Kernel.</p>
<p><span class="math notranslate nohighlight">\(H\)</span>: Hadamard function is a square symmetric matrix of <span class="math notranslate nohighlight">\(1\)</span> and  <span class="math notranslate nohighlight">\(-1\)</span>
where each column is orthogonal. <code class="docutils literal notranslate"><span class="pre">torchfry</span></code> comes with three options for Hadamard:</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">Matmul</span></code> explicitly builds a Hadamard matrix of size <span class="math notranslate nohighlight">\(d \times d\)</span> and
stores it to memory, which will be used for matrix multiplication of the input
against this matrix to achieve the Hadamard transform. This implementation is
extremely quick when leveraging strong GPUs, but requires storing a large matrix.</p>
<p><code class="docutils literal notranslate"><span class="pre">Dao</span></code> is adapted from <a class="reference external" href="https://github.com/Dao-AILab/fast-hadamard-transform">https://github.com/Dao-AILab/fast-hadamard-transform</a>,
which is written in CUDA and provides a PyTorch interface to leverage GPU power
even further.</p>
<p><code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> repeatedly splits the input tensor into pairs, sums and subtracts
them, and concatenates the results step-by-step, applying the Hadamard transform
efficiently without explicitly creating the matrix.</p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(G\)</span>:
Diagonal Gaussian matrix. Data sampled from a normal distribution with variance
proportional to the dimension of the input data.</p>
<p><span class="math notranslate nohighlight">\(\Pi\)</span>:
Applies a permutation to randomize the order of the rows. After the second
Hadamard is applied, the rows are independent of one another.</p>
<p><span class="math notranslate nohighlight">\(B\)</span>:
Diagonal binary matrix, drawn from a <span class="math notranslate nohighlight">\(\{-1,+1\}\)</span>, helps input data become dense.</p>
<p>When nonlinearity is used, the layer is computed as:</p>
<div class="math notranslate nohighlight">
\[\cos(Vx + u)\]</div>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Le, Q., Sarlós, T., &amp; Smola, A. (2018). Fastfood: Approximate Kernel Expansions in Loglinear Time.
<a class="reference external" href="https://arxiv.org/abs/1408.3060">https://arxiv.org/abs/1408.3060</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<p>A simple example of the Fastfood layer on a linear regression dataset with noise.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torchfry.transforms</span> <span class="kn">import</span> <span class="n">FastfoodLayer</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Linear regression with noise</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FastfoodLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learn_B</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_G</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_S</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">hadamard</span><span class="o">=</span><span class="s2">&quot;Torch&quot;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">FastfoodLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learn_B</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_G</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">learn_S</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">hadamard</span><span class="o">=</span><span class="s2">&quot;Torch&quot;</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training loop for 10 epochs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># model.train()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="go">Epoch [1/10], Loss: 14.2901</span>
<span class="go">Epoch [2/10], Loss: 14.2365</span>
<span class="go">Epoch [3/10], Loss: 14.2638</span>
<span class="go">Epoch [4/10], Loss: 14.2251</span>
<span class="go">Epoch [5/10], Loss: 14.2316</span>
<span class="go">Epoch [6/10], Loss: 14.0655</span>
<span class="go">Epoch [7/10], Loss: 14.0691</span>
<span class="go">Epoch [8/10], Loss: 14.0698</span>
<span class="go">Epoch [9/10], Loss: 14.0007</span>
<span class="go">Epoch [10/10], Loss: 13.9704</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchfry.transforms.FastfoodLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/FastfoodLayer.html#FastfoodLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.FastfoodLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the Fastfood transform to the input tensor following the Fastfood
formula from the cited paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor of shape (batch_size, input_dim).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>x</strong> – Transformed tensor of shape (batch_size, output_dim) after projection,
optionally passed through a cosine-based nonlinearity if enabled.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchfry.transforms.FastfoodLayer.new_feature_map">
<span class="sig-name descname"><span class="pre">new_feature_map</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/FastfoodLayer.html#FastfoodLayer.new_feature_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.FastfoodLayer.new_feature_map" title="Link to this definition"></a></dt>
<dd><p>Sample new permutation and scaling matrices for the Fastfood feature map.</p>
<p>This function initializes the permutation matrix <span class="math notranslate nohighlight">\(P\)</span>, the binary scaling
matrix <span class="math notranslate nohighlight">\(B\)</span>, the Gaussian scaling matrix <span class="math notranslate nohighlight">\(G\)</span>, and the scaling matrix
<span class="math notranslate nohighlight">\(S\)</span> based on the learnable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dtype</strong> (<em>torch.dtype</em>) – Specifies the precision of the floats.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchfry.transforms.FastfoodLayer.phi">
<span class="sig-name descname"><span class="pre">phi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/FastfoodLayer.html#FastfoodLayer.phi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.FastfoodLayer.phi" title="Link to this definition"></a></dt>
<dd><p>Apply random Fourier feature mapping using cosine transformation:</p>
<div class="math notranslate nohighlight">
\[\cos(Vx + u)\]</div>
<p>This operation adds a random phase shift to the input tensor and applies
a cosine nonlinearity, effectively projecting the data into a randomized
feature space for kernel approximation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor that will be transformed.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>x</strong> – Output tensor of the same shape after normalization.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="torchfry.transforms.RKSLayer">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchfry.transforms.</span></span><span class="sig-name descname"><span class="pre">RKSLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learn_G</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nonlinearity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/RKSLayer.html#RKSLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.RKSLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implementation of the Random Kitchen Sink layer for efficient random feature mapping.</p>
<p>This layer approximates a dense random projection using the Random Kitchen Sink
algorithm, which utilizes a random Gaussian matrix. The layer explicitly builds a
dense matrix of random Gaussian noise for this. If no nonlinearity is applied, this
is simply a linear layer. The scale is matched to FastfoodLayer as well.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<em>int</em>) – The input data feature dimension. (<span class="math notranslate nohighlight">\(d\)</span>)</p></li>
<li><p><strong>output_dim</strong> (<em>int</em>) – The output dimension to be projected into. (<span class="math notranslate nohighlight">\(n\)</span>)</p></li>
<li><p><strong>scale</strong> (<em>float</em>) – Scalar factor for normalization. (<span class="math notranslate nohighlight">\(\sigma\)</span>)</p></li>
<li><p><strong>learn_G</strong> (<em>bool</em>) – If True, allows the random Gaussian matrix <span class="math notranslate nohighlight">\(G\)</span> to be learnable.</p></li>
<li><p><strong>nonlinearity</strong> (<em>bool</em>) – If True, apply nonlinearity of <span class="math notranslate nohighlight">\(cos(Vx + u)\)</span>.</p></li>
<li><p><strong>device</strong> (<em>torch.device</em>) – The device on which computations will be performed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<p>Rahimi, A. &amp; Recht, B. (2007). Random features for large-scale kernel machines.
<a class="reference external" href="https://dl.acm.org/doi/10.5555/2981562.2981710">https://dl.acm.org/doi/10.5555/2981562.2981710</a></p>
</aside>
</aside>
<p class="rubric">Examples</p>
<p>A simple example of the Random Kitchen Sink layer on a linear regression dataset with noise.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torchfry.transforms</span> <span class="kn">import</span> <span class="n">RKSLayer</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Linear regression with noise</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">RKSLayer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learn_G</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">RKSLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">learn_G</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training loop for 10 epochs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># model.train()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">], Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="go">Epoch [1/10], Loss: 13.3238</span>
<span class="go">Epoch [2/10], Loss: 13.1642</span>
<span class="go">Epoch [3/10], Loss: 13.3305</span>
<span class="go">Epoch [4/10], Loss: 12.9485</span>
<span class="go">Epoch [5/10], Loss: 13.1686</span>
<span class="go">Epoch [6/10], Loss: 12.8200</span>
<span class="go">Epoch [7/10], Loss: 13.2698</span>
<span class="go">Epoch [8/10], Loss: 12.9570</span>
<span class="go">Epoch [9/10], Loss: 13.0187</span>
<span class="go">Epoch [10/10], Loss: 13.1325</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchfry.transforms.RKSLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/RKSLayer.html#RKSLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.RKSLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Applies the Random Kitchen Sink transform to the input tensor by performing
matrix multiplication against the random Gaussian matrix, optionally followed
by cosine nonlinearity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor of shape (batch_size, input_dim).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>X</strong> – Transformed tensor of shape (batch_size, output_dim) after projection,
optionally passed through a cosine-based nonlinearity if enabled.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchfry.transforms.RKSLayer.phi">
<span class="sig-name descname"><span class="pre">phi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torchfry/transforms/RKSLayer.html#RKSLayer.phi"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchfry.transforms.RKSLayer.phi" title="Link to this definition"></a></dt>
<dd><p>Apply random Fourier feature mapping using cosine transformation:</p>
<div class="math notranslate nohighlight">
\[\cos(Vx + u)\]</div>
<p>This operation adds a random phase shift to the input tensor and applies
a cosine nonlinearity, effectively projecting the data into a randomized
feature space for kernel approximation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.tensor</em>) – Input tensor that will be transformed.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>x</strong> – Output tensor of the same shape after normalization.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchfry.networks.html" class="btn btn-neutral float-left" title="torchfry.networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Robert Bates, Kameron Decker Harris, Jed Christian Pagcaliwagan, Joshua Sonnen.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>